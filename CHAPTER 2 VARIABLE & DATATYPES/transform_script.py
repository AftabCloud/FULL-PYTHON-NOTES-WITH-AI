from pyspark.sql import SparkSession
from pyspark.sql.functions import col, trim, upper, when

# ===================================
# 1Ô∏è‚É£ Initialize Spark session
# ===================================
spark = SparkSession.builder \
    .appName("SalesDataCleaningAndTransformation") \
    .getOrCreate()

# ===================================
# 2Ô∏è‚É£ Load data from S3
# ===================================
input_path = "s3://aftab-emr-practice-bucket/sales_data_dirty.csv"
output_path = "s3://aftab-emr-practice-bucket/output/clean_data"

df = spark.read.option("header", True).csv(input_path)

print("=== üßæ Original Data ===")
df.show(truncate=False)

# ===================================
# 3Ô∏è‚É£ Data Cleaning & Transformation
# ===================================

# üîπ Trim spaces and standardize text cases
df = df.withColumn("region", trim(upper(col("region")))) \
       .withColumn("product", trim(upper(col("product")))) \
       .withColumn("sales", trim(col("sales")))

# üîπ Handle null or empty sales values
df = df.withColumn(
    "sales",
    when((col("sales").isNull()) | (col("sales") == ""), None).otherwise(col("sales"))
)

# üîπ Convert sales column to integer safely
df = df.withColumn("sales", col("sales").cast("int"))

# üîπ Drop rows where essential fields are missing
df_cleaned = df.dropna(subset=["region", "sales", "product"])

# üîπ Remove duplicate records
df_cleaned = df_cleaned.dropDuplicates()

# ===================================
# 4Ô∏è‚É£ Optional Extra Transformations
# ===================================

# Remove negative sales (if any)
df_cleaned = df_cleaned.filter(col("sales") >= 0)

# Replace any unknown or missing region/product names
df_cleaned = df_cleaned.withColumn("region", when(col("region") == "", "UNKNOWN").otherwise(col("region")))
df_cleaned = df_cleaned.withColumn("product", when(col("product") == "", "UNKNOWN").otherwise(col("product")))

# ===================================
# 5Ô∏è‚É£ Save Final Cleaned Data (Single File)
# ===================================

# Coalesce(1) ensures only one CSV file (clean_data.csv)
df_cleaned.coalesce(1) \
    .write.mode("overwrite") \
    .option("header", True) \
    .csv(output_path)

print("‚úÖ Data cleaned and transformation completed successfully!")
print(f"üìÅ Cleaned file saved to: {output_path}")

# ===================================
# 6Ô∏è‚É£ Stop Spark session
# ===================================
spark.stop()
